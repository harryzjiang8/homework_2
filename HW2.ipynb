{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Regularized least squares, a.k.a. \"Ridge regression\"\n",
    "\n",
    "## Note on the autograder:\n",
    "\n",
    "\n",
    "For this homework, you will have access to Numpy, Matplotlib, Seaborn, and Plotly. If you use a different import, it probably will not work!\n",
    "\n",
    "## Follow best practices in your code\n",
    "\n",
    "While we won't grade your homeworks based on these subjective criteria, you should start getting practice following these guidelines as (a) we will be checking for these when we grade your project code and (b) these good programming habits will serve you very well in the future.\n",
    "\n",
    "1. All functions should include informative docstrings.\n",
    "2. You should include type hints for both the inputs and outputs of each new function you define.\n",
    "3. Include short and informative inline comments throughout your code.\n",
    "\n",
    "You will often find your solutions easier to write if you implement helper functions in addition to the required autograded functions in the template `.py` files. Feel free to add these to the same modules alongside the autograded functions.\n",
    "\n",
    "Finally, **do not modify arguments passed into the autograded functions**. Such functions that mutate their inputs have what are called *side effects*: these symptoms can include extremely subtle bugs in other regions of your code and autograder crashes!\n",
    "\n",
    "## Submission:\n",
    "\n",
    "For this homework, you will submit your completed copy of this `.ipynb` notebook as well as `ridge.py`. In particular, see the Application section in this notebook: **you should fill out all of the empty cells to receive full credit**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Many real-world problems can be written as a least squares problem (for example, linear regression):\n",
    "$$\n",
    "\\min_{x\\in\\mathbb{R}^d} \\; \\|Ax-y\\|_2^2,\n",
    "$$\n",
    "(Here, $\\|\\cdot\\|_2$ denotes the Euclidean norm, e.g., for a vector $z\\in\\mathbb{R}^m$, $\\|z\\|_2^2 = \\sum_{i=1}^m z_i^2$.)\n",
    "\n",
    "In the equation above, the matrix `A` has shape `(m, d)` and the vector `y` has shape `(m,)` (in math, we would usually say it's $m \\times 1$, but in `numpy`, the convention is generally to flatten the array to only have one axis).\n",
    "\n",
    "Least squares can be numerically unstable, especially when `A` is ill-conditioned. Adding a small penalty on the size of the coefficients improves numerical stability and can reduce overfitting by discouraging overly large weights. This is formalized by the regularized least squares (ridge regression) problem:\n",
    "$$\n",
    "\\min_{x\\in\\mathbb{R}^d} \\; \\|Ax-y\\|_2^2 + \\lambda\\,\\|x\\|_2^2,\\quad \\lambda>0.\n",
    "$$\n",
    "You can read more here: https://en.wikipedia.org/wiki/Ridge_regression.\n",
    "\n",
    "In this assignment, you will write a solver for the regularized least squares problem.\n",
    "\n",
    "Like standard least squares, the regularized least squares problem has a closed-form solution, and we can also use gradient descent (GD) or stochastic gradient descent (SGD) instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical background\n",
    "\n",
    "**Explicit formula:** The solution to the regularized least squares problem is\n",
    "$$\n",
    " x = (A^\\top A + \\lambda I_d)^{-1}A^\\top y,\n",
    "$$\n",
    "where `I_d` is the identity matrix of shape `(d, d)`. In other words, the optimal solution comes from the normal equation\n",
    "$$\n",
    " (A^\\top A + \\lambda I_d) x = A^\\top y.\n",
    "$$\n",
    "\n",
    "**GD:** To implement GD, we use the gradient of the objective function\n",
    "$$\n",
    " f(x) = \\|Ax-y\\|_2^2 + \\lambda \\|x\\|_2^2,\\quad \\nabla f(x) = 2(A^\\top A + \\lambda I_d) x - 2A^\\top y.\n",
    "$$\n",
    "\n",
    "**SGD:** Computing the gradient using all samples can be expensive. To speed this up, use a minibatch of `b` samples to approximate the gradient:\n",
    "$$\n",
    " \\nabla f(x) \\approx 2(A_b ^\\top A_b + \\lambda I_d) x - 2A_b^\\top y_b,\n",
    "$$\n",
    "where `A_b` is obtained by randomly selecting `b` rows from `A` (keeping all columns), and `y_b` contains the corresponding entries from `y`.\n",
    "\n",
    "Note: gradient descent (GD) is exactly the special case of SGD with `batch_size = m`. When `b = m`, we have $A_b = A$ and $y_b = y$, so the minibatch gradient matches the full gradient and the two methods are identical.\n",
    "\n",
    "## Coding instructions:\n",
    "\n",
    "Implement a solver for the regularized least squares problem in `ridge.py` (your renamed template file) by creating a class `LinearSolver` with the following behavior and method signatures. We have provided a template to get you started. Your implementation will be graded on these exact behaviors, defaults, and error messages, using the test cases listed farther below.\n",
    "\n",
    "- `__init__(A, y)`\n",
    "  - Inputs: `A` (2D numpy array with shape `(m, d)`), `y` (1D numpy array with shape `(m,)`).\n",
    "  - Behavior:\n",
    "    - If `A` is not 2D, raise `ValueError: Initialization failed because the A matrix is not a 2D numpy array.`\n",
    "    - If `y` is not 1D, print `Warning: vector is not a 1D numpy array. Reshaped to 1D numpy array automatically.` and reshape to 1D automatically.\n",
    "    - If `A.shape[0] != y.shape[0]`, raise `ValueError: Mismatched number of rows. Initialization failed.`\n",
    "\n",
    "- `rls(reg)` - closed form solution of regularized least squares\n",
    "  - `reg` (float | None): regularization strength ($\\lambda$). Defaults to `1.0`.\n",
    "  - Cases and required errors:\n",
    "    - `reg < 0`: raise `ValueError: Regularization parameter should be nonnegative or None.`\n",
    "    - `reg == 0` or `reg is None`: solve the unregularized least-squares problem using `np.linalg.solve`.\n",
    "    - `reg > 0`: solve the ridge regression problem using `np.linalg.solve`.\n",
    "  - Returns: coefficient vector `x` as a `numpy` array of shape `(d,)`.\n",
    "\n",
    "- `sgd(reg, max_iter, batch_size, step_size)` - GD or SGD solution of regularized least squares\n",
    "  - `reg` (nonnegative float): ridge penalty strength ($\\lambda$). If `reg < 0`, raise `ValueError: Regularization parameter should be nonnegative or None.`\n",
    "  - `max_iter` (int): maximum number of gradient descent steps. Default `100`.\n",
    "  - `batch_size` (int): minibatch size (rows sampled without replacement each update). Default `2`. When `batch_size = m`, this is fullâ€‘batch GD.\n",
    "  - `step_size` (float): learning rate ($\\eta$). Default `0.01`.\n",
    "  - Initialization: start from the zero vector `x = np.zeros(d)`.\n",
    "  - Each step, perform the update with minibatch `(A_b, y_b)`: $$x \\leftarrow x - \\eta\\,\\big(2A_b^\\top(A_b x - y_b) + 2\\lambda x\\big)$$\n",
    "  - Returns: coefficient vector `x` as a `numpy` array of shape `(d,)`.\n",
    "\n",
    "Docstrings should clearly explain the shapes of numpy arrays used by your class and methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ridge import LinearSolver\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Initialization failed because the A matrix is not a 2D numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,))\n\u001b[1;32m      3\u001b[0m reg \u001b[38;5;241m=\u001b[39m LinearSolver(A, y)\n\u001b[0;32m----> 4\u001b[0m reg\u001b[38;5;241m.\u001b[39mrls(reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/projects/homework_2/ridge.py:33\u001b[0m, in \u001b[0;36mLinearSolver.rls\u001b[0;34m(self, reg)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03mReturn coefficients of vector x as np.ndarry of shape (d,)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m:raises ValueError: if reg is negative\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reg \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization failed because the A matrix is not a 2D numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reg \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reg \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(A\u001b[38;5;241m.\u001b[39mT\u001b[38;5;129m@A\u001b[39m, A\u001b[38;5;241m.\u001b[39mT\u001b[38;5;129m@y\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Initialization failed because the A matrix is not a 2D numpy array."
     ]
    }
   ],
   "source": [
    "A = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "y = np.random.normal(loc=0, scale=1, size=(10,))\n",
    "reg = LinearSolver(A, y)\n",
    "reg.rls(reg=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "Your code will be graded based on the following test cases. Many of these tests have to do with validating the inputs, as it's really easy to make a subtle error such as passing the wrong size matrices for $A$ or $y$. (They should be easy to code up, though, since you don't actually need to run the regression for invalid inputs!)\n",
    "\n",
    "#### Matrix shape with wrong number of axes is not accepted\n",
    "\n",
    "If `A` is not a 2D numpy array, your constructor should raise `ValueError: Initialization failed because the A matrix is not a 2D numpy array.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Initialization failed because the A matrix is not a 2D numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m40\u001b[39m,))\n\u001b[0;32m----> 4\u001b[0m reg \u001b[38;5;241m=\u001b[39m LinearSolver(A, y)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/Teaching/PIC16/PIC16B/Fall25/HW/HW2/autograder/solution/ridge.py:27\u001b[0m, in \u001b[0;36mLinearSolver.__init__\u001b[0;34m(self, A, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, A: np\u001b[38;5;241m.\u001b[39mndarray, y: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(A) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialization failed because the A matrix is not a 2D numpy array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA \u001b[38;5;241m=\u001b[39m A\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(y) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Initialization failed because the A matrix is not a 2D numpy array."
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "A = np.random.normal(loc=0, scale=1, size=(10, 20, 10))\n",
    "y = np.random.normal(loc=0, scale=1, size=(40,))\n",
    "reg = LinearSolver(A, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector reshaping warning\n",
    "\n",
    "If `y` is not 1D, print the warning and reshape `y` to a 1D vector automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vector is not a 1D numpy array. Reshaped to 1D numpy array automatically.\n"
     ]
    }
   ],
   "source": [
    "A = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "y = np.random.normal(loc=0, scale=1, size=(10, 1))\n",
    "reg = LinearSolver(A, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization error: mismatched rows\n",
    "\n",
    "If the number of rows in `A` and the length of `y` do not match, raise `ValueError: \"Mismatched number of rows. Initialization failed.`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: vector is not a 1D numpy array. Reshaped to 1D numpy array automatically.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatched number of rows. Initialization failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m reg \u001b[38;5;241m=\u001b[39m LinearSolver(A, y)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/Teaching/PIC16/PIC16B/Fall25/HW/HW2/autograder/solution/ridge.py:35\u001b[0m, in \u001b[0;36mLinearSolver.__init__\u001b[0;34m(self, A, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMismatched number of rows. Initialization failed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatched number of rows. Initialization failed."
     ]
    }
   ],
   "source": [
    "A = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "y = np.random.normal(loc=0, scale=1, size=(30, 2))\n",
    "reg = LinearSolver(A, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RLS error: negative regularization\n",
    "\n",
    "If `reg < 0`, `rls` should raise a `ValueError` with the specified message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Regularization parameter should be nonnegative or None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,))\n\u001b[1;32m      3\u001b[0m reg \u001b[38;5;241m=\u001b[39m LinearSolver(A, y)\n\u001b[0;32m----> 4\u001b[0m reg\u001b[38;5;241m.\u001b[39mrls(reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/projects/homework_2/ridge.py:35\u001b[0m, in \u001b[0;36mLinearSolver.rls\u001b[0;34m(self, reg)\u001b[0m\n\u001b[1;32m     33\u001b[0m ATy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA\u001b[38;5;241m.\u001b[39mT\u001b[38;5;129m@self\u001b[39m\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reg \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegularization parameter should be nonnegative or None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reg \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reg \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(ATA, ATy)\n",
      "\u001b[0;31mValueError\u001b[0m: Regularization parameter should be nonnegative or None."
     ]
    }
   ],
   "source": [
    "A = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "y = np.random.normal(loc=0, scale=1, size=(10,))\n",
    "reg = LinearSolver(A, y)\n",
    "reg.rls(reg=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Regularization parameter should be nonnegative or None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,))\n\u001b[1;32m      3\u001b[0m reg \u001b[38;5;241m=\u001b[39m LinearSolver(A, y)\n\u001b[0;32m----> 4\u001b[0m reg\u001b[38;5;241m.\u001b[39mrls(reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/Teaching/PIC16/PIC16B/Fall25/HW/HW2/autograder/solution/ridge.py:65\u001b[0m, in \u001b[0;36mLinearSolver.rls\u001b[0;34m(self, reg)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(ATA, ATy)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reg \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegularization parameter should be nonnegative or None.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(ATA \u001b[38;5;241m+\u001b[39m reg \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(d), ATy)\n",
      "\u001b[0;31mValueError\u001b[0m: Regularization parameter should be nonnegative or None."
     ]
    }
   ],
   "source": [
    "A = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "y = np.random.normal(loc=0, scale=1, size=(10,))\n",
    "reg = LinearSolver(A, y)\n",
    "reg.rls(reg=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RLS with default parameters\n",
    "\n",
    "Calling `rls()` with no arguments should default to lambda=1 and return the corresponding ridge solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0. ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0], [0, 0]])\n",
    "y = np.array([1, 1])\n",
    "reg = LinearSolver(A, y)\n",
    "reg.rls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0. ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0], [0, 0]])\n",
    "y = np.array([1, 1])\n",
    "reg = LinearSolver(A, y)\n",
    "reg.rls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RLS without regularization\n",
    "\n",
    "Calling `rls` with lambda=0 or lambda=None should return the unregularized least squares solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([1, 1, 2])\n",
    "reg = LinearSolver(A, y)\n",
    "reg.rls(reg=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([1, 1, 2])\n",
    "reg = LinearSolver(A, y)\n",
    "reg.rls(reg=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.33333333, 1.33333333])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([1, 1, 3])\n",
    "reg = LinearSolver(A, y)\n",
    "reg.rls(reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.33333333, 1.33333333])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([1, 1, 3])\n",
    "reg = LinearSolver(A, y)\n",
    "reg.rls(reg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD error: negative regularization\n",
    "\n",
    "If `reg < 0`, `sgd` should raise `ValueError: Regularization parameter should be nonnegative or None.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Regularization parameter should be nonnegative or None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,))\n\u001b[1;32m      3\u001b[0m reg \u001b[38;5;241m=\u001b[39m LinearSolver(A, y)\n\u001b[0;32m----> 4\u001b[0m reg\u001b[38;5;241m.\u001b[39msgd(reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/projects/homework_2/ridge.py:56\u001b[0m, in \u001b[0;36mLinearSolver.sgd\u001b[0;34m(self, reg, max_iter, batch_size, step_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Stochastic Gradient Descent\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03mraises ValueError: if reg is negative\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reg \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegularization parameter should be nonnegative or None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m m, d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(d)\n",
      "\u001b[0;31mValueError\u001b[0m: Regularization parameter should be nonnegative or None."
     ]
    }
   ],
   "source": [
    "A = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "y = np.random.normal(loc=0, scale=1, size=(10,))\n",
    "reg = LinearSolver(A, y)\n",
    "reg.sgd(reg=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Regularization parameter should be nonnegative or None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,))\n\u001b[1;32m      3\u001b[0m reg \u001b[38;5;241m=\u001b[39m LinearSolver(A, y)\n\u001b[0;32m----> 4\u001b[0m reg\u001b[38;5;241m.\u001b[39msgd(reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/Teaching/PIC16/PIC16B/Fall25/HW/HW2/autograder/solution/ridge.py:108\u001b[0m, in \u001b[0;36mLinearSolver.sgd\u001b[0;34m(self, reg, max_iter, batch_size, step_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mStochastic gradient descent (SGD) for ridge regression.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    ValueError: Regularization parameter should be nonnegative or None.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reg \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegularization parameter should be nonnegative or None.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m m, d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((d,))\n",
      "\u001b[0;31mValueError\u001b[0m: Regularization parameter should be nonnegative or None."
     ]
    }
   ],
   "source": [
    "A = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "y = np.random.normal(loc=0, scale=1, size=(10,))\n",
    "reg = LinearSolver(A, y)\n",
    "reg.sgd(reg=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have different outputs for the next two test examples since SGD randomly \n",
    "selects training samples. For grading, your SGD solution is compared to a reference solution to check that the deviation is not statistically significant. In addition, the correct functioning of the SGD method will be relevant to the Application section below.\n",
    "\n",
    "#### SGD (no regularization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 2.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0], [0, 0], [0, 0]])\n",
    "y = np.array([1, 1, 1])\n",
    "reg = LinearSolver(A, y)\n",
    "reg.sgd(reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69637434, 0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0], [0, 0], [0, 0]])\n",
    "y = np.array([1, 1, 1])\n",
    "reg = LinearSolver(A, y)\n",
    "reg.sgd(reg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD (with regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0], [0, 0], [0, 0]])\n",
    "y = np.array([1, 1, 1])\n",
    "reg = LinearSolver(A, y)\n",
    "reg.sgd(reg=1, max_iter=100, batch_size=3, step_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0. ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0], [0, 0], [0, 0]])\n",
    "y = np.array([1, 1, 1])\n",
    "reg = LinearSolver(A, y)\n",
    "reg.sgd(reg=1, max_iter=100, batch_size=3, step_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "Regularization is a crucial technique in machine learning. In this part, you will use your solver to compare least squares (call `rls` with lambda=0) and ridge regression (call `rls` with lambda>0), and then repeat with `sgd`.\n",
    "\n",
    "The following code generates training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(19)\n",
    "\n",
    "m = 100                                               # number of training samples\n",
    "d = 100                                               # number of features\n",
    "A = np.random.normal(0, 1, size=(m, d))               # training data matrix\n",
    "x_true = np.random.uniform(size=(d,))                 # true coefficient vector\n",
    "y = A @ x_true + np.random.normal(0, 0.5, size=(m,))  # training labels\n",
    "\n",
    "m_test = 500                                          # number of test samples \n",
    "A_test = np.random.normal(0, 1, size=(m_test, d))     # test data matrix\n",
    "y_test = A_test @ x_true                              # test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares solution (closed form)\n",
    "Find the least squares solution without regularization using training data $A$ and $y$ by calling `LinearSolver.rls` with lambda=0 to get $x_{LS}$.\n",
    "\n",
    "Report two quantities:\n",
    "1. The norm $\\|x_{true} - x_{LS}\\|$.\n",
    "2. The mean squared error on the test set. Compute $y_{pred} = A_{test}\\, x_{LS}$, then compute $MSE = \\frac{1}{n}\\sum_i (y_{test,i} - y_{pred,i})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm: 75.86377961234408\n",
      "Mean Squared Error: 2907214.4792334177\n"
     ]
    }
   ],
   "source": [
    "reg = LinearSolver(A, y)\n",
    "x_LS = reg.rls(0)\n",
    "print(\"norm:\", np.linalg.norm(x_true - x_LS))\n",
    "y_pred = A_test@x_LS\n",
    "MSE = np.sum(np.square(y_test - y_pred))\n",
    "print(\"Mean Squared Error:\", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression (closed form)\n",
    "Find a regularized solution using training data $A$ and $y$ by calling `LinearSolver.rls` with lambda>0 to get $x_{RLS}$.\n",
    "\n",
    "Report two quantities, analogous to the least squares case:\n",
    "1. $\\|x_{true} - x_{RLS}\\|$.\n",
    "2. Test-set MSE for $x_{RLS}$.\n",
    "\n",
    "Repeat this experiment for 3 values of $\\lambda$ (e.g., $0.1, 1.0, 10.0$) and briefly state which you selected and why (based on test-set MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm: 1.8621738686559521\n",
      "Mean Squared Error: 1573.4598025855078\n"
     ]
    }
   ],
   "source": [
    "reg = LinearSolver(A, y)\n",
    "x_RLS = reg.rls(1.0)\n",
    "print(\"norm:\", np.linalg.norm(x_true - x_RLS))\n",
    "y_pred = A_test@x_RLS\n",
    "MSE = np.sum(np.square(y_test - y_pred))\n",
    "print(\"Mean Squared Error:\", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected $\\lambda = 1.0$ since it resulted in the lowest MSE of about $1573$ whereas both $\\lambda = 0.1$ and $\\lambda = 10$ gave a MSE of over $2000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat with SGD\n",
    "\n",
    "Using `LinearSolver.sgd`, compute:\n",
    "- An unregularized solution (use lambda=0); report the norm $\\|x_{true} - x_{\\text{SGD, ls}}\\|$ and the test-set MSE.\n",
    "- A regularized solution (use your best lambda); report the norm $\\|x_{true} - x_{\\text{SGD, ridge}}\\|$ and the test-set MSE.\n",
    "\n",
    "Notes:\n",
    "- You may need to try different max_iter, batch_size, and step_size to get good performance. Briefly state your chosen values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm: 5.503951433592289\n",
      "Mean Squared Error: 14413.9611978744\n"
     ]
    }
   ],
   "source": [
    "# Unregularized solution\n",
    "reg = LinearSolver(A, y)\n",
    "x_SGDls = reg.sgd(reg=0, step_size=1e-4)\n",
    "print(\"norm:\", np.linalg.norm(x_true - x_SGDls))\n",
    "y_pred = A_test@x_SGDls\n",
    "MSE = np.sum(np.square(y_test - y_pred))\n",
    "print(\"Mean Squared Error:\", MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm: 5.480878220661539\n",
      "Mean Squared Error: 14074.954405999011\n"
     ]
    }
   ],
   "source": [
    "# Regularized Solution\n",
    "reg = LinearSolver(A, y)\n",
    "x_SGDridge = reg.sgd(reg=1, step_size=1e-4)\n",
    "print(\"norm:\", np.linalg.norm(x_true - x_SGDridge))\n",
    "y_pred = A_test@x_SGDridge\n",
    "MSE = np.sum(np.square(y_test - y_pred))\n",
    "print(\"Mean Squared Error:\", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does your experiment support that regularization is helpful in improving the model's performance? (Write answer below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
